

#### Take a backup of the etcd cluster and save it to /opt/etcd-backup.db.

Weight: 10

```
controlplane ~ ➜  export ETCDCTL_API=3

controlplane ~ ➜  etcdctl snapshot -h

controlplane ~ ✖ ls /etc/kubernetes/pki/etcd/ca.crt  
ca.key  healthcheck-client.crt  healthcheck-client.key  peer.crt  peer.key  server.crt  server.key

controlplane ~ ➜  etcdctl snapshot save --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --endpoints=127.0.0.1:2379 --key=/etc/kubernetes/pki/etcd/server.key  /opt/etcd-backup.db

Test our backup of file
ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshotdb

Restore etcd from a backup file:
ETCDCTL_API=3 etcdctl snapshot restore tmp/etcd-backup.db  --data-dir /var/lib/etcd-backup --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key

``` 


#### Create a Pod called redis-storage with image: redis:alpine with a Volume of type emptyDir that lasts for the life of the Pod.

Weight: 10

#### Specs on the below. /  Pod named 'redis-storage' created /   Pod 'redis-storage' uses Volume type of emptyDir / Pod 'redis-storage' uses volumeMount with mountPath = /data/redis

``` 
controlplane ~ ✖ kubectl apply -f empty_pod.yaml 
pod/redis-storage created

controlplane ~ ➜  cat empty_pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: redis-storage
spec:
  containers:
  - image: redis:alpine
    name: redis-storage
    volumeMounts:
    - mountPath: /data/redis
      name: redis-storage
  volumes:
  - name: redis-storage
    emptyDir: {}

controlplane ~ ➜  kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
redis-storage   1/1     Running   0          50s

controlplane ~ ✖ kubectl exec --stdin --tty redis-storage -- /bin/sh
/data # 

/data # df -h 
Filesystem                Size      Used Available Use% Mounted on
overlay                 969.0G    196.6G    772.4G  20% /
tmpfs                   102.3G         0    102.3G   0% /proc/acpi
udev                    102.3G         0    102.3G   0% /proc/keys
udev                    102.3G         0    102.3G   0% /proc/timer_list
udev                    102.3G         0    102.3G   0% /proc/sched_debug
tmpfs                   102.3G         0    102.3G   0% /proc/scsi
tmpfs                    64.0M         0     64.0M   0% /dev
tmpfs                   102.3G         0    102.3G   0% /sys/fs/cgroup
.                       969.0G    196.6G    772.4G  20% /data
/dev/sda1               969.0G    196.6G    772.4G  20% /data/redis
/dev/sda1               969.0G    196.6G    772.4G  20% /etc/hosts
/dev/sda1               969.0G    196.6G    772.4G  20% /dev/termination-log
.                       969.0G    196.6G    772.4G  20% /etc/hostname
.                       969.0G    196.6G    772.4G  20% /etc/resolv.conf
shm                      64.0M         0     64.0M   0% /dev/shm
tmpfs                   204.5G     12.0K    204.5G   0% /run/secrets/kubernetes.io/serviceaccount
udev                    102.3G         0    102.3G   0% /dev/null
udev                    102.3G         0    102.3G   0% /dev/random
udev                    102.3G         0    102.3G   0% /dev/full
udev                    102.3G         0    102.3G   0% /dev/tty
udev                    102.3G         0    102.3G   0% /dev/zero
udev                    102.3G         0    102.3G   0% /dev/urandom
udev                    102.3G         0    102.3G   0% /proc/kcore
tmpfs                   102.3G         0    102.3G   0% /sys/firmware

/data # uname -a
Linux redis-storage 5.4.0-1092-gcp #101~18.04.1-Ubuntu SMP Mon Oct 17 18:29:06 UTC 2022 x86_64 Linux
/data # ls -lh  /data/redis
total 0      
/data # exit
```

#### Create a new pod called super-user-pod with image busybox:1.28. Allow the pod to be able to set system_time.
Weight: 8
#### The container should sleep for 4800 seconds. / Pod: super-user-pod /  Container Image: busybox:1.28 /SYS_TIME capabilities for the conatiner?

```
controlplane ~ ➜  kubectl run super-user-pod --image=busybox:1.28 --dry-run=client -o yaml --command -- sleep 4800 > super-user-pod.yaml

controlplane ~ ➜  vim super-user-pod.yaml 

controlplane ~ ➜  kubectl apply -f super-user-pod.yaml 
pod/super-user-pod created

controlplane ~ ➜  kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
redis-storage    1/1     Running   0          3m24s
super-user-pod   1/1     Running   0          4s

controlplane ~ ➜  cat super-user-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: super-user-pod
  name: super-user-pod
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: busybox:1.28
    name: super-user-pod
    securityContext:
      capabilities:
        add: ["SYS_TIME"]
```

#### A pod definition file is created at /root/CKA/use-pv.yaml. Make use of this manifest file and mount the persistent volume called pv-1. Ensure the pod is running and the PV is bound.

Weight: 12

#### mountPath: /data / persistentVolumeClaim Name: my-pvc /  persistentVolume Claim configured correctly /  pod using the correct mountPath /  pod using the persistent volume claim?
``` 
ontrolplane ~ ➜  kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv-1   10Mi       RWO            Retain           Available                                   17m

controlplane ~ ➜  cat pvc
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi

controlplane ~ ➜  kubectl apply -f pvc 
persistentvolumeclaim/my-pvc created

controlplane ~ ➜  kubectl get pvc
NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
my-pvc   Bound    pv-1     10Mi       RWO                           4s


controlplane ~ ➜  kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE
pv-1   10Mi       RWO            Retain           Bound    default/my-pvc                           19m

controlplane ~ ➜  cat CKA/use-pv.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: use-pv
  name: use-pv
spec:
  containers:
  - image: nginx
    name: use-pv
    volumeMounts:
      - mountPath: "/data"
        name: use-pv
  volumes:
    - name: use-pv
      persistentVolumeClaim:
        claimName: my-pvc

controlplane ~ ➜  vim CKA/use-pv.yaml 

controlplane ~ ➜  kubectl apply -f CKA/use-pv.yaml 
pod/use-pv created

controlplane ~ ➜  kubectl get pods
NAME             READY   STATUS              RESTARTS   AGE
redis-storage    1/1     Running             0          30m
super-user-pod   1/1     Running             0          27m
use-pv           0/1     ContainerCreating   0          5s

controlplane ~ ➜  kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
redis-storage    1/1     Running   0          31m
super-user-pod   1/1     Running   0          27m
use-pv           1/1     Running   0          8s

controlplane ~ ➜  kubectl describe pod use-pv 
Name:         use-pv
Namespace:    default
Priority:     0
Node:         node01/10.6.145.12
Start Time:   Tue, 01 Nov 2022 05:17:06 +0000
Labels:       run=use-pv
Annotations:  <none>
Status:       Running
IP:           10.50.192.3
IPs:
  IP:  10.50.192.3
Containers:
  use-pv:
    Container ID:   containerd://2bfd01d7a66c76fa1c8036d885d63add2a67cc55dc99ed1a685fa0bbe47d05d9
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:943c25b4b66b332184d5ba6bb18234273551593016c0e0ae906bab111548239f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 01 Nov 2022 05:17:12 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /data from mypd (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sfphr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  mypd:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  my-pvc
    ReadOnly:   false
  kube-api-access-sfphr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  23s   default-scheduler  Successfully assigned default/use-pv to node01
  Normal  Pulling    22s   kubelet            Pulling image "nginx"
  Normal  Pulled     17s   kubelet            Successfully pulled image "nginx" in 4.808326483s
  Normal  Created    17s   kubelet            Created container use-pv
  Normal  Started    17s   kubelet            Started container use-pv
``` 
  
##### Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. Next upgrade the deployment to version 1.17 using rolling update.

Weight: 15

##### Deployment : nginx-deploy. Image: nginx:1.16 / Image: nginx:1.16 / Task: Upgrade the version of the deployment to 1:17 /  Task: Record the changes for the image upgrade

``` 
controlplane ~ ➜  kubectl create deployment nginx-deploy --image=nginx:1.16 --replicas=1
deployment.apps/nginx-deploy created

controlplane ~ ➜  kubectl set image deployment/nginx-deploy nginx=nginx:1.17 --record 
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/nginx-deploy image updated

controlplane ~ ➜  kubectl rollout status deployment/nginx-deploy
deployment "nginx-deploy" successfully rolled out

controlplane ~ ➜  kubectl rollout history deployment nginx-deploy 
deployment.apps/nginx-deploy 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment/nginx-deploy nginx=nginx:1.17 --record=true

controlplane ~ ➜  kubectl describe deployments.apps nginx-deploy 
Name:                   nginx-deploy
Namespace:              default
CreationTimestamp:      Tue, 01 Nov 2022 05:21:03 +0000
Labels:                 app=nginx-deploy
Annotations:            deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: kubectl set image deployment/nginx-deploy nginx=nginx:1.17 --record=true
Selector:               app=nginx-deploy
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx-deploy
  Containers:
   nginx:
    Image:        nginx:1.17
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-deploy-75d47c9469 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  99s   deployment-controller  Scaled up replica set nginx-deploy-7cfd4975d6 to 1
  Normal  ScalingReplicaSet  54s   deployment-controller  Scaled up replica set nginx-deploy-75d47c9469 to 1
  Normal  ScalingReplicaSet  47s   deployment-controller  Scaled down replica set nginx-deploy-7cfd4975d6 to 0
  
  ``` 

#### Create a new user called john. Grant him access to the cluster. John should have permission to create, list, get, update and delete pods in the development namespace . The private key exists in the location: /root/CKA/john.key and csr at /root/CKA/john.csr.

#### Important Note: As of kubernetes 1.19, the CertificateSigningRequest object expects a signerName.
Weight: 15
#### Please refer the documentation to see an example. The documentation tab is available at the top right of terminal.

#### CSR: john-developer Status:Approved / Role Name: developer, namespace: development, Resource: Pods /  Access: User 'john' has appropriate permissions

```
controlplane ~ ➜  cat CKA/john.csr | base64 | tr -d "\n"
LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZEQ0NBVHdDQVFBd0R6RU5NQXNHQTFVRUF3d0VhbTlvYmpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRApnZ0VQQURDQ0FRb0NnZ0VCQUxub1JGemFaM2t5dmcrWks5eGJJL3VHWEF2M0srWHhvc3RCVm9iNDhGY2JCVkp5CkFuQzIrYXpjNEpzYmdiVCtUbFE3M1pHbnlhTGhDNnZ1b0kyUnZ3TzdXOUdvZFhhUURodUpoRlcweEtlTnFNL0cKcWRJS0F5cDhzQjk0WGRBRytBblcwWDZMV1JFYXZiRVc0eTAwa0dlaG9mTGlWUHhTRWFEZmpTSFVNcUthNHZPYQpxZ3Y2cWFGM2h4dE1uT2JCeHRHQncxSkwveWZkMUcvSDl1aEF5MVdYYSsrK2s3MWk4d2Y5RlpVa091MFNsa1ZlCmlROVZlNDZOTVlKVGlvNTluQUZ0aFRMb0FyRzVqMTBvZm5jdmFWelBsekQzWTE4MjhFQjFld1F2RU0vUVNKVTMKc0N1Tk93SzVRd2wyVlU3SExvcC9sbE1ZZStYRGVPUWhjOGMxQVRjQ0F3RUFBYUFBTUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRQjJTVlVTRlVwTUgyM2kycWxZeUJ5QllzQWU1NTZ3bnpJbVMzUk1neTliR0xlY2xhampCNHZECklUTlNFdkczTHhmWXVvZGRCSkFaaDZSRHZjTUx4SkNvd2hVWDlnWElSZGxrQUthV25kTHppRG03aVltYXhNVDUKbzRaQ2hscHNCWW1aZk5wcDZzQkRkazh6WjhjRjkvS29TRTFOeWQ1bzA1WXZKb25CbUoyUVg2ODdPQW9CeE1YawpPN09ITHhCZGx3Z2ZNMEo4VStMYXFhU0pURGpVZ3k2TXpmUmpkbTZlVVFDY0piQ3gzZHBhL01qMmdSSnMrMElwCnJpaG9QZHYvaVNwaDl3aGNiVVk5RzBsdngxOU1DUHRlakJJbEcvVUc5VVA5c1IybzB3aTRrTjNCTHluNDRsbmMKYjBsT2V6bTI1NHE2d1I4Z3dGRG1pQUxuaHZmVDlGNGwKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==

controlplane ~ ➜  cat john.csr 
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: john-developer
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZEQ0NBVHdDQVFBd0R6RU5NQXNHQTFVRUF3d0VhbTlvYmpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRApnZ0VQQURDQ0FRb0NnZ0VCQUxub1JGemFaM2t5dmcrWks5eGJJL3VHWEF2M0srWHhvc3RCVm9iNDhGY2JCVkp5CkFuQzIrYXpjNEpzYmdiVCtUbFE3M1pHbnlhTGhDNnZ1b0kyUnZ3TzdXOUdvZFhhUURodUpoRlcweEtlTnFNL0cKcWRJS0F5cDhzQjk0WGRBRytBblcwWDZMV1JFYXZiRVc0eTAwa0dlaG9mTGlWUHhTRWFEZmpTSFVNcUthNHZPYQpxZ3Y2cWFGM2h4dE1uT2JCeHRHQncxSkwveWZkMUcvSDl1aEF5MVdYYSsrK2s3MWk4d2Y5RlpVa091MFNsa1ZlCmlROVZlNDZOTVlKVGlvNTluQUZ0aFRMb0FyRzVqMTBvZm5jdmFWelBsekQzWTE4MjhFQjFld1F2RU0vUVNKVTMKc0N1Tk93SzVRd2wyVlU3SExvcC9sbE1ZZStYRGVPUWhjOGMxQVRjQ0F3RUFBYUFBTUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRQjJTVlVTRlVwTUgyM2kycWxZeUJ5QllzQWU1NTZ3bnpJbVMzUk1neTliR0xlY2xhampCNHZECklUTlNFdkczTHhmWXVvZGRCSkFaaDZSRHZjTUx4SkNvd2hVWDlnWElSZGxrQUthV25kTHppRG03aVltYXhNVDUKbzRaQ2hscHNCWW1aZk5wcDZzQkRkazh6WjhjRjkvS29TRTFOeWQ1bzA1WXZKb25CbUoyUVg2ODdPQW9CeE1YawpPN09ITHhCZGx3Z2ZNMEo4VStMYXFhU0pURGpVZ3k2TXpmUmpkbTZlVVFDY0piQ3gzZHBhL01qMmdSSnMrMElwCnJpaG9QZHYvaVNwaDl3aGNiVVk5RzBsdngxOU1DUHRlakJJbEcvVUc5VVA5c1IybzB3aTRrTjNCTHluNDRsbmMKYjBsT2V6bTI1NHE2d1I4Z3dGRG1pQUxuaHZmVDlGNGwKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth

controlplane ~ ➜  kubectl create -f john.csr 
certificatesigningrequest.certificates.k8s.io/john created

controlplane ~ ➜  kubectl get csr 
NAME        AGE   SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION
csr-5n4rp   42m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:ndzci7    <none>              Approved,Issued
csr-m9fp8   42m   kubernetes.io/kube-apiserver-client-kubelet   system:node:controlplane   <none>              Approved,Issued
john        8s    kubernetes.io/kube-apiserver-client           kubernetes-admin           <none>              Pending

controlplane ~ ➜  kubectl certificate approve john-developer
certificatesigningrequest.certificates.k8s.io/john-developer approved

controlplane ~ ➜  kubectl get csr
NAME             AGE     SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION
csr-5n4rp        46m     kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:ndzci7    <none>              Approved,Issued
csr-m9fp8        47m     kubernetes.io/kube-apiserver-client-kubelet   system:node:controlplane   <none>              Approved,Issued
john-developer   2m27s   kubernetes.io/kube-apiserver-client           kubernetes-admin           <none>              Approved,Issued

controlplane ~ ➜  kubectl create role developer --verb=create,list,get,update,delete --resource=pods -n development --dry-run=client -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: null
  name: developer
  namespace: development
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - create
  - list
  - get
  - update
  - delete

controlplane ~ ➜  kubectl create role developer --verb=create,list,get,update,delete --resource=pods -n development role.rbac.authorization.k8s.io/developer created

controlplane ~ ➜  kubectl get role -n development 
NAME        CREATED AT
developer   2022-11-02T05:45:22Z

controlplane ~ ➜  kubectl describe role -n development 
Name:         developer
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 []              [create list get update delete]
  
  controlplane ~ ➜  kubectl create rolebinding john-developer --role=developer -n development --user=john 
rolebinding.rbac.authorization.k8s.io/john-developer created


controlplane ~ ➜  kubectl get role -n development 
NAME        CREATED AT
developer   2022-11-02T05:45:22Z

controlplane ~ ➜  kubectl get rolebindings.rbac.authorization.k8s.io -n development 
NAME             ROLE             AGE
john-developer   Role/developer   25s

controlplane ~ ➜  kubectl auth can-i get pods -n development --as john
yes

controlplane ~ ➜  kubectl auth can-i list pods -n development --as john
yes

controlplane ~ ➜  kubectl auth can-i create pods -n development --as john
yes

controlplane ~ ➜  kubectl auth can-i watch pods -n development --as john
no
``` 

#### Create a nginx pod called nginx-resolver using image nginx, expose it internally with a service called nginx-resolver-service. Test that you are able to look up the service and pod names from within the cluster. Use the image: busybox:1.28 for dns lookup. Record results in /root/CKA/nginx.svc and /root/CKA/nginx.pod
Weight: 15
#### Pod: nginx-resolver created /  Service DNS Resolution recorded correctly /  Pod DNS resolution recorded correctly

```
controlplane ~ ➜  kubectl run nginx-resolver --image=nginx 
pod/nginx-resolver created

controlplane ~ ➜  kubectl expose pod nginx-resolver --port=80 --name=nginx-resolver-service
service/nginx-resolver-service exposed

controlplane ~ ➜  kubectl run buzybox --image=busybox:1.28 --command -- sleep 4800 
pod/buzybox created

controlplane ~ ➜  kubectl exec buzybox -- date
Sat Nov  5 08:19:41 UTC 2022

controlplane ~ ➜  kubectl exec buzybox -- nslookup nginx-resolver
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'nginx-resolver'
command terminated with exit code 1

controlplane ~ ✖ kubectl exec buzybox -- nslookup nginx-resolver-service
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      nginx-resolver-service
Address 1: 10.107.65.170 nginx-resolver-service.default.svc.cluster.local

controlplane ~ ➜  kubectl exec buzybox -- nslookup nginx-resolver-service > /root/CKA/nginx.svc

controlplane ~ ➜  kubectl get pod -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
buzybox                         1/1     Running   0          57s   10.50.192.6   node01   <none>           <none>
nginx-deploy-75d47c9469-8szpq   1/1     Running   0          24m   10.50.192.5   node01   <none>           <none>
nginx-resolver                  1/1     Running   0          11m   10.50.192.4   node01   <none>           <none>
redis-storage                   1/1     Running   0          34m   10.50.192.1   node01   <none>           <none>
super-user-pod                  1/1     Running   0          30m   10.50.192.2   node01   <none>           <none>
use-pv                          1/1     Running   0          27m   10.50.192.3   node01   <none>           <none>


controlplane ~ ✖ kubectl exec buzybox -- nslookup 10-50-192-4.default.pod.cluster.local
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      10-50-192-4.default.pod.cluster.local
```

##### Create a static pod on node01 called nginx-critical with image nginx and make sure that it is recreated/restarted automatically in case of a failure.

Weight: 15

#### Use /etc/kubernetes/manifests as the Static Pod path for example. / static pod configured under /etc/kubernetes/manifests ? /  Pod nginx-critical-node01 is up and running

```
controlplane ~ ➜  kubectl run nginx-critical --image=nginx --restart='Always' --dry-run=client -o yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-critical
  name: nginx-critical
spec:
  containers:
  - image: nginx
    name: nginx-critical
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

controlplane ~ ➜  ssh node01
Last login: Thu Nov 24 05:48:50 2022 from 10.7.48.10

root@node01 ~ ➜  cd /etc/kubernetes/manifests/

root@node01 /etc/kubernetes/manifests ➜  ls -lh
total 0

root@node01 /etc/kubernetes/manifests ➜  vim nginx-critical.yml

root@node01 /etc/kubernetes/manifests ➜  exit
logout
Connection to node01 closed.

controlplane ~ ➜  kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
buzybox                         1/1     Running   0          8m21s
nginx-critical-node01           1/1     Running   0          9s
nginx-deploy-75d47c9469-6z44m   1/1     Running   0          26m
nginx-resolver                  1/1     Running   0          10m
redis-storage                   1/1     Running   0          43m
super-user-pod                  1/1     Running   0          40m
use-pv                          1/1     Running   0          31m

```
