#### Create a new service account with the name pvviewer. Grant this Service account access to list all PersistentVolumes in the cluster by creating an appropriate cluster role called pvviewer-role and ClusterRoleBinding called pvviewer-role-binding.
#### Next, create a pod called pvviewer with the image: redis and serviceAccount: pvviewer in the default namespace.
Weight: 12
#### ServiceAccount: pvviewer /  ClusterRole: pvviewer-role / ClusterRoleBinding: pvviewer-role-binding /Pod: pvviewer / Pod configured to use ServiceAccount pvviewer ?

```
controlplane ~ ➜  kubectl create serviceaccount pvviewer
serviceaccount/pvviewer created

controlplane ~ ➜  kubectl create clusterrole pvviewer-role --verb=list --resource=PersistentVolumes
clusterrole.rbac.authorization.k8s.io/pvviewer-role created

controlplane ~ ➜  kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=pvviewer
error: serviceaccount must be <namespace>:<name>

controlplane ~ ✖ kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer
clusterrolebinding.rbac.authorization.k8s.io/pvviewer-role-binding created

kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<namespace>:<serviceaccountname> [-n <namespace>]

controlplane ~ ✖ kubectl auth can-i list PersistentVolumes --as system:serviceaccount:default:pvviewer
Warning: resource 'persistentvolumes' is not namespace scoped
yes

controlplane ~ ➜  kubectl auth can-i create PersistentVolumes --as system:serviceaccount:default:pvviewer
Warning: resource 'persistentvolumes' is not namespace scoped
no

controlplane ~ ➜  kubectl run pvviewer --image=redis --dry-run=client -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pvviewer
  name: pvviewer
spec:
  containers:
  - image: redis
    name: pvviewer
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

controlplane ~ ➜  kubectl run pvviewer --image=redis --dry-run=client -o yaml > serviceaccount_pod.yaml

controlplane ~ ➜  vim serviceaccount_pod.yaml 

controlplane ~ ➜  kubectl create -f serviceaccount_pod.yaml 
pod/pvviewer created

controlplane ~ ➜  kubectl describe pods pvviewer 
Name:         pvviewer
Namespace:    default
Priority:     0
Node:         node01/10.41.165.12
Start Time:   Tue, 08 Nov 2022 13:20:21 +0000
Labels:       run=pvviewer
Annotations:  <none>
Status:       Running
IP:           10.50.192.1
IPs:
  IP:  10.50.192.1
Containers:
  pvviewer:
    Container ID:   containerd://92dd2a0d24790865a85bb6b57a571dd83257f7621ea9ca251090d499cc1f5b3d
    Image:          redis
    Image ID:       docker.io/library/redis@sha256:aeed51f49a6331df0cb2c1039ae3d1d70d882be3f48bde75cd240452a2348e88
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 08 Nov 2022 13:20:29 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vlzgg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vlzgg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  19s   default-scheduler  Successfully assigned default/pvviewer to node01
  Normal  Pulling    18s   kubelet            Pulling image "redis"
  Normal  Pulled     11s   kubelet            Successfully pulled image "redis" in 6.579092119s
  Normal  Created    11s   kubelet            Created container pvviewer
  Normal  Started    11s   kubelet            Started container pvviewer
  
```
#### List the InternalIP of all nodes of the cluster. Save the result to a file /root/CKA/node_ips.

#### Answer should be in the format: InternalIP of controlplane<space>InternalIP of node01 (in a single line)

``` 
controlplane ~ ➜  kubectl get nodes -o json |jq -c paths
["apiVersion"]
["items"]
["items",0]
["items",0,"apiVersion"]
["items",0,"kind"]
["items",0,"metadata"]
["items",0,"metadata","annotations"]
["items",0,"metadata","annotations","kubeadm.alpha.kubernetes.io/cri-socket"]
["items",0,"metadata","annotations","node.alpha.kubernetes.io/ttl"]
["items",0,"metadata","annotations","volumes.kubernetes.io/controller-managed-attach-detach"]
["items",0,"metadata","creationTimestamp"]
["items",0,"metadata","labels"]
["items",0,"metadata","labels","beta.kubernetes.io/arch"]
["items",0,"metadata","labels","beta.kubernetes.io/os"]
["items",0,"metadata","labels","kubernetes.io/arch"]
["items",0,"metadata","labels","kubernetes.io/hostname"]
["items",0,"metadata","labels","kubernetes.io/os"]
["items",0,"metadata","labels","node-role.kubernetes.io/control-plane"]
["items",0,"metadata","labels","node.kubernetes.io/exclude-from-external-load-balancers"]
["items",0,"metadata","name"]
["items",0,"metadata","resourceVersion"]
["items",0,"metadata","uid"]
["items",0,"spec"]
["items",0,"spec","podCIDR"]
["items",0,"spec","podCIDRs"]
["items",0,"spec","podCIDRs",0]

 kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'
```

  
#### Create a pod called multi-pod with two containers.
####Container 1, name: alpha, image: nginx
#### Container 2: name: beta, image: busybox, command: sleep 4800
 Weight: 12

Environment Variables:
container 1:
name: alpha

Container 2:
name: beta

Pod Name: multi-pod

Container 1: alpha

Container 2: beta

Container beta commands set correctly?

Container 1 Environment Value Set

Container 2 Environment Value Set

``` 

kubectl run multi-pod --image=buzybox --dry-run=client -o yaml --command -- sleep 4800 > multipad.yam
  
---
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  containers:
  - image: nginx
    name: alpha
    env:
    - name: name
      value: alpha
  - image: busybox
    name: beta
    command: ["sleep", "4800"]
    env:
    - name: name
      value: beta

 ``` 
#### Create a Pod called non-root-pod , image: redis:alpine
#### runAsUser: 1000 / fsGroup: 2000
Weight: 8

Pod non-root-pod fsGroup configured
Pod non-root-pod runAsUser configured
  
 ```
  controlplane ~ ➜  kubectl run non-root-pod --image=redis:alpine --dry-run=client -o yaml > non-root.yaml

controlplane ~ ✖ vim non-root.yaml

controlplane ~ ➜  kubectl create -f non-root.yaml
pod/non-root-pod created

controlplane ~ ➜  cat non-root.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: non-root-pod
  name: non-root-pod
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
  containers:
  - image: redis:alpine
    name: non-root-pod

controlplane ~ ➜  kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
multi-pod      2/2     Running   0          9m32s
non-root-pod   1/1     Running   0          12s
``` 
  
#### We have deployed a new pod called np-test-1 and a service called np-test-service. Incoming connections to this service are not working. Troubleshoot and fix it.
#### Create NetworkPolicy, by the name ingress-to-nptest that allows incoming connections to the service over port 80.

#### Important: Don't delete any current objects deployed.

#### Important: Don't Alter Existing Objects!
  
  Weight: 14

#### NetworkPolicy: Applied to All sources (Incoming traffic from all pods)?

#### NetWorkPolicy: Correct Port?

#### NetWorkPolicy: Applied to correct Pod?

```
controlplane ~ ➜  kubectl get pods 
NAME           READY   STATUS    RESTARTS   AGE
multi-pod      2/2     Running   0          3m55s
non-root-pod   1/1     Running   0          31s
np-test-1      1/1     Running   0          12s
pvviewer       1/1     Running   0          28m


controlplane ~ ➜  kubectl get svc -o wide
NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE     SELECTOR
kubernetes        ClusterIP   10.96.0.1       <none>        443/TCP   45m     <none>
np-test-service   ClusterIP   10.101.137.32   <none>        80/TCP    3m48s   run=np-test-1

  
controlplane ~ ➜  kubectl run new --image=alpine/curl --rm -it -- sh
If you don't see a command prompt, try pressing enter.
/ # curl np-test-service
^C
/ # 


controlplane ~ ➜  kubectl describe svc np-test-service
Name:              np-test-service
Namespace:         default
Labels:            run=np-test-1
Annotations:       <none>
Selector:          run=np-test-1
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.106.178.66
IPs:               10.106.178.66
Port:              <unset>  80/TCP
TargetPort:        80/TCP
Endpoints:         10.50.192.1:80
Session Affinity:  None
Events:            <none>

controlplane ~ ➜  vim ingress.yml

controlplane ~ ➜  kubectl create -f ingress.yml 
networkpolicy.networking.k8s.io/ingress-to-nptest created


# cat ingress-to-nptest.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  policyTypes:
    - Ingress
  ingress:
    - ports:
      - protocol: TCP
        port: 80
  
#kubectl create -f ingress-to-nptest.yml
  
controlplane ~ ➜  kubectl run new --image=alpine/curl --rm -it -- sh 
If you don't see a command prompt, try pressing enter.
/ # curl np-test-service
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
/ # 
``` 
#### Taint the worker node node01 to be Unschedulable. Once done, create a pod called dev-redis, image redis:alpine, to ensure workloads are not scheduled to this worker node. Finally, create a new pod called prod-redis and image: redis:alpine with toleration to be scheduled on node01.

#### key: env_type, value: production, operator: Equal and effect: NoSchedule

####Key = env_type

####Value = production

####Effect = NoSchedule

####pod 'dev-redis' (no tolerations) is not scheduled on node01?

#### Create a pod 'prod-redis' to run on node01  

```
controlplane ~ ➜  kubectl taint nodes node01 env_type=production:NoSchedule
node/node01 tainted

controlplane ~ ➜  kubectl run dev-redis --image=redis:alpine 
pod/dev-redis created

controlplane ~ ➜  kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
dev-redis   1/1     Running   0          5s
np-test-1   1/1     Running   0          9m7s


controlplane ~ ✖ kubectl get pods -o wide
NAME        READY   STATUS    RESTARTS   AGE     IP            NODE           NOMINATED NODE   READINESS GATES
dev-redis   1/1     Running   0          34s     10.50.0.4     controlplane   <none>           <none>
np-test-1   1/1     Running   0          9m36s   10.50.192.1   node01         <none>           <none>

controlplane ~ ➜  kubectl run prod-redis --image=redis:alpine --dry-run=client -o yaml > taint_tolerte.yaml

controlplane ~ ➜  vim taint_tolerte.yaml 
  
controlplane ~ ➜  cat taint_tolerte.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: prod-redis
  name: prod-redis
spec:
  containers:
  - image: redis:alpine
    name: prod-redis
  tolerations:
  - key: "env_type"
    value: "production"
    operator: "Equal"
    effect: "NoSchedule"

controlplane ~ ➜  kubectl create -f taint_tolerte.yaml 
pod/prod-redis created

controlplane ~ ➜  kubectl get pods
NAME         READY   STATUS    RESTARTS   AGE
dev-redis    1/1     Running   0          5m20s
np-test-1    1/1     Running   0          14m
prod-redis   1/1     Running   0          6s

controlplane ~ ➜  kubectl get pods -o wide
NAME         READY   STATUS    RESTARTS   AGE     IP            NODE           NOMINATED NODE   READINESS GATES
dev-redis    1/1     Running   0          5m23s   10.50.0.4     controlplane   <none>           <none>
np-test-1    1/1     Running   0          14m     10.50.192.1   node01         <none>           <none>
prod-redis   1/1     Running   0          9s      10.50.192.2   node01         <none>           <none>
  
```
  
#### Create a pod called hr-pod in hr namespace belonging to the production environment and frontend tier .
image: redis:alpine

#### Use appropriate labels and create all the required objects if it does not exist in the system already.
Weight: 8
#### hr-pod labeled with environment production?

#### hr-pod labeled with tier frontend?
  
  ``` 
  controlplane ~ ➜  kubectl get ns
NAME              STATUS   AGE
default           Active   26m
kube-node-lease   Active   26m
kube-public       Active   26m
kube-system       Active   26m

controlplane ~ ➜  kubectl create ns hr
namespace/hr created

controlplane ~ ➜  kubectl run hr-pod --image=redis:alpine --labels="environment=production,tier=frontend" -n hr
pod/hr-pod created

controlplane ~ ➜  kubectl get pods -n hr
NAME         READY   STATUS    RESTARTS   AGE
dev-redis    1/1     Running   0          9m44s
hr-pod       1/1     Running   0          5s
np-test-1    1/1     Running   0          18m
prod-redis   1/1     Running   0          4m30s
  
controlplane ~ ➜  kubectl describe pod hr-pod -n hr 
Name:         hr-pod
Namespace:    hr
Priority:     0
Node:         controlplane/10.12.93.12
Start Time:   Mon, 21 Nov 2022 06:36:54 +0000
Labels:       environment=production
              tier=frontend
Annotations:  <none>
Status:       Running
IP:           10.50.0.5
IPs:
  IP:  10.50.0.5
Containers:
  hr-pod:
    Container ID:   containerd://a01908490a50acc0b9380ede80eec61f9104ab483a60069e61a68374cb7ce478
    Image:          redis:alpine
    Image ID:       docker.io/library/redis@sha256:2700d5097763fda285c463f4eefc3d0730a2df2a9d48e66707b19d5a5e5f23d4
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Mon, 21 Nov 2022 06:36:55 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4cvs9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-4cvs9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  34s   default-scheduler  Successfully assigned hr/hr-pod to controlplane
  Normal  Pulled     33s   kubelet            Container image "redis:alpine" already present on machine
  Normal  Created    33s   kubelet            Created container hr-pod
  Normal  Started    33s   kubelet            Started container hr-pod
```
  
#### A kubeconfig file called super.kubeconfig has been created under /root/CKA. There is something wrong with the configuration. Troubleshoot and fix it.
Weight: 8
#### Fix /root/CKA/super.kubeconfig
  
  controlplane ~ ➜  kubectl get nodes --kubeconfig=/root/CKA/super.kubeconfig 
The connection to the server controlplane:9999 was refused - did you specify the right host or port?
  
controlplane ~ ➜  cat .kube/config 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1URXlNVEEyTURZeU5sb1hEVE15TVRFeE9EQTJNRFl5Tmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTG1MCjlWaDQ1MkxYVlNDMlpVazZFZjN4REVVMmM4YTN3OUlTZUxtQjJuV1BCOCtGRXNYcXNsd202L2hXUi9uTTlCR2IKTnI3Zms5UGF5MVlTVXhNbmkvVGxNYjhLOERoRXM2OTZ0UHp3UmVRZTZmMFRQN1NZUE1nVUdOSGNnZm05ZTV3UApFcmVFRnVQWUZ0VTRDSDF0VFB1Q1BkcWtuOU82QVZHRlZrZXRNRFZDTG9KOXRHWkNIeWc3cmdNREoyUkJHalpsCmgveW1WNFdoN2RxQldDYzBKd0JKZG1uL0U1bGVWWGQ4NTVmaGVzejIyWWw5d2tzcHJRTkNQZ0RYL3RBV3AyT0MKb2ZLRWdsbFJpNmJpZld1UkhOdTBkMlAxWTZGaWdIODZvVDlHc1ZUSFUweUhRUlBaVmRaSUluamQrRjA2N3VmdAp6aXNWL08zOUZOWCtkV2ZMR1A4Q0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZGVFNkeE1XY2gyVHExTFZOQzhwVDJhL3RsbE1NQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQXZGUk16VmIwVVpCYVcvME9hZwpJYU81Y2hPK3ppeWd2Sm45My8vYmlzRFgwdEt1Ni9wVVZ1R2d1R3AwMVE4VzFLTTJaUThQVm5lL1JVNVJBaGRGCmo4THFLaXZkNi8zN040V0xad1JmYUJiaDdpSjVaZDl1NFZ0MGJSZGI4WC8xQlFTS3NzbzhVYlhuQnliRjRNdlAKVWJKK3A4STZYTEY2Z2U1bDBVeXJZZEpnbzYwVW9vbnlqSjQrU21tSTlkNnFpQ045VjRLK3p4cU5PaGVMMnFvSQpqVllXKzUrM1Blb2ZaL214TnkvSTZDUEprbmk0SmdjSmJVWDRreVlPMmQvT1E5a2VwNk9HelBQR2pJSmg2ZTZnCnlGWGtqYmVSMUVMZUMwb2l2V0E1S2cvZTFzcjdjN05GU3BXN3YwWERzMUx4eWwrMmNneTRWaGR2MEZEaFREdG0KSjRVPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://controlplane:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:

controlplane ~ ✖ vim /root/CKA/super.kubeconfig      -- 6443

controlplane ~ ➜  kubectl get nodes --kubeconfig=/root/CKA/super.kubeconfig 
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   35m   v1.24.0
node01         Ready    <none>          34m   v1.24.0
``` 
  
#### We have created a new deployment called nginx-deploy. scale the deployment to 3 replicas. Has the replica's increased? Troubleshoot the issue and fix it.

#### deployment has 3 replicas
  
